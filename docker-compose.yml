#version: '3.8'

services:
  spark-master:
    build: ./spark-cluster
    deploy:
      resources:
        limits:
          memory: 4g
          cpus: '2.0'
    container_name: spark-master
    ports:
      - 9010:8080
      - 1001:7077
      - 1000:10000
      - 1005:4041
      - 1006:4040
      - 8998:8998
      - 18080:18080
    volumes:
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-master:/opt/hadoop/data/dataNode
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Master..."
        /usr/local/share/spark/sbin/start-master.sh;

        # echo "Starting livy Server..."
        # /usr/local/share/livy/bin/livy-server

        until hdfs dfsadmin -safemode get | grep -q "Safe mode is OFF"; do
          echo "HDFS safe mode is ON, waiting..."
          sleep 5
        done
        echo "HDFS safe mode is OFF, starting thrift server..."

        echo "Starting Thirft Server..."
        /usr/local/share/spark/sbin/start-thriftserver.sh --master yarn --deploy-mode client --name ThriftServer \
          --conf spark.sql.warehouse.dir=hdfs://namenode:8020/user/airflow/dbt_output \
          --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --conf spark.executorEnv.PYSPARK_PYTHON=python3 \
          --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
          --conf spark.sql.catalog.hudi=org.apache.spark.sql.hudi.catalog.HoodieCatalog \
          --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
          --conf spark.sql.catalog.spark_catalog.type=hive \
          --conf spark.sql.catalog.spark_catalog.uri=thrift://metastore:9083 \
          --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.iceberg.type=hive \
          --conf spark.sql.catalog.iceberg.uri=thrift://metastore:9083 \
          --conf spark.sql.catalog.iceberg.warehouse=hdfs://namenode:8020/user/airflow/dbt_output \
          --conf spark.executor.memory=512m \
          --conf spark.executor.cores=1 \
          --conf spark.executor.instances=1 \
          --files hdfs://namenode:8020/user/spark/jars/spark/pyspark.zip,hdfs://namenode:8020/user/spark/jars/spark/py4j-0.10.9.9-src.zip
          # --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
          # --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \
          # --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog \
          # --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
          # --conf spark.sql.catalog.local.type=hadoop \
          # --conf spark.sql.catalog.local.warehouse=hdfs://namenode:8020/user/airflow/dbt_output \
          # --conf spark.sql.catalog.spark_catalog.type=hive \
          # --conf spark.hadoop.hive.metastore.warehouse.dir=hdfs://namenode:8020/user/airflow/dbt_output \
        
        echo "Starting Spark History Server..."
        /usr/local/share/spark/sbin/start-history-server.sh

        tail -f /usr/local/hadoop/logs/* &
        wait
      '
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://namenode:8020/user/airflow/spark-logs"
    depends_on:
      - namenode
    networks:
      - my_shared_network

  spark-worker-1:
    build: ./spark-cluster
    container_name: spark-worker-1
    restart: always
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    ports:
      - 9044:8080
      - 9055:7077
      - 9066:10000
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./spark-cluster/spark-jobs/:/home/
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-worker-1:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network

  spark-worker-2:
    build: ./spark-cluster
    container_name: spark-worker-2
    restart: always
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    ports:
      - 9071:8080
      - 9088:7077
      - 9099:10000
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-worker-2:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network
  
  namenode:
    build: ./hadoop_installation/
    container_name: namenode
    hostname: namenode
    volumes:
      - ./hadoop_installation/hadoop_data:/hadoop_data
      - ./hadoop_installation/hadoop_namenode:/opt/hadoop/data/nameNode
    user: root
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    ports:
      - "9870:9870"
      - 8088:8088
      - 8020:8020
    networks:
      - my_shared_network
  
  postgres:
    image: postgres
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: metastore_db
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: password
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: '0.5'
    ports:
      - 11003:5432
    # volumes:
    #   - ./postgres/postgres_data:/var/lib/postgresql/data
    volumes:
      - ./postgres/postgres_data:/var/lib/postgresql
    networks:
      - my_shared_network

  metastore:
    image: apache/hive:4.0.0
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore' 
      # SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password'
      SERVICE_OPTS: >-
        -Xmx1G
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=password
    ports:
      - '9083:9083'
    volumes:
      - type: bind
        source: ./jars/postgresql-42.6.0.jar
        target: /opt/hive/lib/postgres.jar
      - ./spark-cluster/hadoop_config/hive-site.xml:/opt/hive/conf/hive-site.xml   
    networks:
      - my_shared_network


networks:
  my_shared_network:
    # driver: bridge
    external: true
