#version: '3.8'

services:
  spark-master:
    build: ./spark-cluster
    container_name: spark-master
    ports:
      - 9010:8080
      - 1001:7077
      - 1000:10000
      - 1005:4041
      - 1006:4040
    volumes:
      # - ./spark-cluster/id_rsa:/root/.ssh/id_rsa
      # - ./spark-cluster/id_rsa.pub:/root/.ssh/id_rsa.pub
      # - ./spark-cluster/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-master:/opt/hadoop/data/dataNode
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-master.sh;

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    depends_on:
      - namenode
    networks:
      - my_shared_network

  spark-worker-1:
    build: ./spark-cluster
    container_name: spark-worker-1
    restart: always
    ports:
      - 9044:8080
      - 9055:7077
      - 9066:10000
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./spark-cluster/spark-jobs/:/home/
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-worker-1:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network

  spark-worker-2:
    build: ./spark-cluster
    container_name: spark-worker-2
    restart: always
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    ports:
      - 9071:8080
      - 9088:7077
      - 9099:10000
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-worker-2:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network
  
  namenode:
    build: ./hadoop_installation/
    container_name: namenode
    hostname: namenode
    volumes:
      - ./hadoop_installation/hadoop_data:/hadoop_data
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./hadoop_installation/hadoop_config/ssh/id_rsa:/root/.ssh/id_rsa
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/id_rsa.pub
      - ./hadoop_installation/hadoop_namenode:/opt/hadoop/data/nameNode
    user: root
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    ports:
      - "9870:9870"
      - 8088:8088
    networks:
      - my_shared_network
  
  postgres:
    image: postgres
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - 11003:5432
    volumes:
      - ./postgres/postgres_data:/var/lib/postgresql/data
    networks:
      - my_shared_network

  metastore:
    image: apache/hive:4.0.0
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password'
    ports:
      - '9083:9083'
    volumes:
      - ./hive-data:/opt/hive/data/warehouse
      - type: bind
        source: ./jars/postgresql-42.6.0.jar
        target: /opt/hive/lib/postgres.jar
    networks:
      - my_shared_network

  hiveserver2:
    image: apache/hive:4.0.0
    restart: unless-stopped
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083'
      IS_RESUME: 'true'
      SERVICE_NAME: hiveserver2
    ports:
      - 10000:10000
      - 10002:10002
    volumes:
      - ./hive-data:/opt/hive/data/warehouse
    networks:
      - my_shared_network

  hue:
    image: gethue/hue
    restart: unless-stopped
    container_name: hue
    ports:
      - 11004:8888
    volumes:
      - ./hue.ini/hue.ini:/usr/share/hue/desktop/conf/hue.ini
      - ./hue.ini/desktop.db:/usr/share/hue/desktop/desktop.db
    networks:
      - my_shared_network

volumes:
  hive-db:
  warehouse:


networks:
  my_shared_network:
    # driver: bridge
    external: true
