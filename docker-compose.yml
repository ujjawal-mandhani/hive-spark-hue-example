#version: '3.8'

services:
  spark-master:
    build: ./spark-cluster
    container_name: spark-master
    ports:
      - 9010:8080
      - 1001:7077
      - 1000:10000
      - 1005:4041
      - 1006:4040
    volumes:
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-master:/opt/hadoop/data/dataNode
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-master.sh;

        echo "Starting Thirft Server..."
        /usr/local/share/spark/sbin/start-thriftserver.sh --master yarn --deploy-mode client --name ThriftServer \
          --conf spark.sql.warehouse.dir=hdfs://namenode:8020/user/airflow/dbt_output \
          --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --conf spark.executorEnv.PYSPARK_PYTHON=python3 \
          --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
          --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog \
          --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension \
          --conf spark.hadoop.hive.metastore.warehouse.dir=hdfs://namenode:8020/user/airflow/dbt_output \
          --files hdfs://namenode:8020/user/spark/jars/spark-3.5.0/pyspark.zip,hdfs://namenode:8020/user/spark/jars/spark-3.5.0/py4j-0.10.9.7-src.zip

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    depends_on:
      - namenode
    networks:
      - my_shared_network

  spark-worker-1:
    build: ./spark-cluster
    container_name: spark-worker-1
    restart: always
    ports:
      - 9044:8080
      - 9055:7077
      - 9066:10000
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./spark-cluster/spark-jobs/:/home/
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-worker-1:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network

  spark-worker-2:
    build: ./spark-cluster
    container_name: spark-worker-2
    restart: always
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    ports:
      - 9071:8080
      - 9088:7077
      - 9099:10000
    depends_on:
      - spark-master
      - namenode
    environment:
      - YARN_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
      - HADOOP_OPTS=--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED
    command: >
      bash -c '
        /usr/sbin/sshd || echo "sshd failed";

        # Wait for Namenode to be reachable
        until nc -z namenode 8020; do
          echo "Waiting for Namenode at namenode:8020..."
          sleep 5
        done

        mkdir -p /usr/local/hadoop/logs /tmp/nm-local-dir

        echo "Starting HDFS DataNode..."
        /usr/local/hadoop/bin/hdfs --daemon start datanode

        echo "Starting YARN NodeManager..."
        yarn --daemon start nodemanager

        echo "Starting Spark Worker..."
        /usr/local/share/spark/sbin/start-worker.sh spark://spark-master:7077

        tail -f /usr/local/hadoop/logs/* &
        wait
      '

    volumes:
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./spark-cluster/spark-jobs/:/home/
      - ./spark-cluster/spark-worker-2:/opt/hadoop/data/dataNode
    networks:
      - my_shared_network
  
  namenode:
    build: ./hadoop_installation/
    container_name: namenode
    hostname: namenode
    volumes:
      - ./hadoop_installation/hadoop_data:/hadoop_data
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/authorized_keys
      - ./hadoop_installation/hadoop_config/ssh/id_rsa:/root/.ssh/id_rsa
      - ./hadoop_installation/hadoop_config/ssh/id_rsa.pub:/root/.ssh/id_rsa.pub
      - ./hadoop_installation/hadoop_namenode:/opt/hadoop/data/nameNode
    user: root
    mem_limit: 3g
    cpus: 0.5
    mem_reservation: 512m
    ports:
      - "9870:9870"
      - 8088:8088
    networks:
      - my_shared_network
  
  postgres:
    image: postgres
    restart: unless-stopped
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: metastore_db
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: password
    ports:
      - 11003:5432
    volumes:
      - ./postgres/postgres_data:/var/lib/postgresql/data
    networks:
      - my_shared_network

  metastore:
    image: apache/hive:4.0.0
    depends_on:
      - postgres
    restart: unless-stopped
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore' 
      # SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password'
      SERVICE_OPTS: >-
        -Xmx1G
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=password
    ports:
      - '9083:9083'
    volumes:
      - type: bind
        source: ./jars/postgresql-42.6.0.jar
        target: /opt/hive/lib/postgres.jar
      - ./spark-cluster/hadoop_config/hive-site.xml:/opt/hive/conf/hive-site.xml   
    networks:
      - my_shared_network
      
  # livy:
  #   image: apache/livy:0.8.0-incubating
  #   container_name: livy
  #   depends_on:
  #     - namenode
  #   ports:
  #     - "8998:8998"
  #   environment:
  #     - LIVY_SPARK_MASTER=yarn
  #     - HADOOP_CONF_DIR=/etc/hadoop/conf
  #   volumes:
  #     - ./spark-cluster/hadoop_config:/etc/hadoop/conf
  #   networks:
  #     - my_shared_network

  # hiveserver2:
  #   image: apache/hive:4.0.0
  #   restart: unless-stopped
  #   container_name: hiveserver2
  #   environment:
  #     HIVE_SERVER2_THRIFT_PORT: 10000
  #     SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083'
  #     IS_RESUME: 'true'
  #     SERVICE_NAME: hiveserver2
  #   ports:
  #     - 10000:10000
  #     - 10002:10002
  #   volumes:
  #     - ./hive-data:/opt/hive/data/warehouse
  #   networks:
  #     - my_shared_network

  # hue:
  #   image: gethue/hue
  #   restart: unless-stopped
  #   container_name: hue
  #   ports:
  #     - 11004:8888
  #   volumes:
  #     - ./hue.ini/hue.ini:/usr/share/hue/desktop/conf/hue.ini
  #     - ./hue.ini/desktop.db:/usr/share/hue/desktop/desktop.db
  #   networks:
  #     - my_shared_network

volumes:
  hive-db:
  warehouse:


networks:
  my_shared_network:
    # driver: bridge
    external: true
